{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predictions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1b3WARMMGu"
      },
      "source": [
        "## 1. Import Statements\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3ucYDf8LhZ0"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xqhnDDL4sY"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BertModel, BertForSequenceClassification"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI4sOx2eL8hR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "26b5ddb8-da98-4ba7-db66-a443a103aa73"
      },
      "source": [
        "# Set up the GPU.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5epSitaMR0M"
      },
      "source": [
        "## 2. Load the Data\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMIVJAgKCqqQ"
      },
      "source": [
        "The original code in this section is located in `bert-training.ipynb`. It is included here to make the `get_star_predictions()` function to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSqOM7iyL6s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42886de1-7ba5-4892-ba69-a0f092a83001"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEit55RnL9lL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "77067a3d-95be-4dd4-8d47-9e05b6efd1c3"
      },
      "source": [
        "github_url = 'https://raw.githubusercontent.com/csbanon/bert-product-rating-predictor/master/data/reviews_comments_stars.csv'\n",
        "df = pd.read_csv(github_url)\n",
        "df = df[['comment', 'stars']]\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I could sit here and write all about the specs...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A very reasonably priced laptop for basic comp...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is the best laptop deal you can get, full...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A few months after the purchase....It is still...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BUYER BE AWARE: This computer has Microsoft 10...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195760</th>\n",
              "      <td>I have not tried this camera without the SD ca...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195761</th>\n",
              "      <td>Hello, I bought this item months ago and I tho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195762</th>\n",
              "      <td>This is an incredible camera for the money!!  ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195763</th>\n",
              "      <td>Great cameras. Purchased some for my mother af...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195764</th>\n",
              "      <td>Just getting it set up and seem s to work well...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>195765 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  comment  stars\n",
              "0       I could sit here and write all about the specs...      5\n",
              "1       A very reasonably priced laptop for basic comp...      4\n",
              "2       This is the best laptop deal you can get, full...      5\n",
              "3       A few months after the purchase....It is still...      5\n",
              "4       BUYER BE AWARE: This computer has Microsoft 10...      1\n",
              "...                                                   ...    ...\n",
              "195760  I have not tried this camera without the SD ca...      5\n",
              "195761  Hello, I bought this item months ago and I tho...      1\n",
              "195762  This is an incredible camera for the money!!  ...      5\n",
              "195763  Great cameras. Purchased some for my mother af...      5\n",
              "195764  Just getting it set up and seem s to work well...      5\n",
              "\n",
              "[195765 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT10SfiKirC5"
      },
      "source": [
        "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=1)\n",
        "test_dataset = test_dataset.reset_index(drop=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyVl2j4UBR2W"
      },
      "source": [
        "## 3. Define the BERT Model\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSUyLlJiDCIU"
      },
      "source": [
        "The original code in this section is located in `bert-training.ipynb`. It is included here to make the `get_star_predictions()` function to work. The output is suppressed to make the notebook easier to read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-gGqXqEMb0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19db51d-eaa0-44e1-d051-bd72e6e6416b"
      },
      "source": [
        "%%capture\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = len(df['stars'].unique()), # Number of unique labels for our multi-class classification problem.\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCq6SJCGBVS_"
      },
      "source": [
        "## 4. Load the Trained Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si9anPkRDGg6"
      },
      "source": [
        "Here, we load the `trained_model.bin` file, which contains the trained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1-Lp6pcNzMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b48c7e-f9eb-4770-d58c-f767e5c396ac"
      },
      "source": [
        "# Load the trained model.\n",
        "model.load_state_dict(torch.load('drive/My Drive/CAP 5610/trained_model.bin'))\n",
        "model.eval()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAszbcKeBaqE"
      },
      "source": [
        "## 5. Define the Reviews Dataset\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66b7TLv1DERl"
      },
      "source": [
        "The original code in this section is located in `star_prediction.ipyn`. It is included here to make the `get_star_predictions()` function to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj3iNQsUOPTQ"
      },
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, df, max_length=512):\n",
        "        self.df = df\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.max_length = max_length \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # input=review, label=stars\n",
        "        review = self.df.loc[idx, 'comment']\n",
        "        # labels are 0-indexed\n",
        "        label = int(self.df.loc[idx, 'stars']) - 1\n",
        "        \n",
        "        encoded = self.tokenizer(\n",
        "            review,                      # Review to encode.\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,  # Truncate all segments to max_length.\n",
        "            padding='max_length',        # Pad all reviews with the [PAD] token to the max_length.\n",
        "            return_attention_mask=True,  # Construct attention masks.\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        input_ids = encoded['input_ids']\n",
        "        attn_mask = encoded['attention_mask']\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids),\n",
        "            'attn_mask': torch.tensor(attn_mask), \n",
        "            'label': torch.tensor(label)\n",
        "        }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E2XPeDeCbwR"
      },
      "source": [
        "## 6. Predict the Star Ratings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pON7R4sCDQ0D"
      },
      "source": [
        "The following code takes a DataFrame containing reviews and returns their predicted star ratings with an accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK5c2JlR-RhR"
      },
      "source": [
        "def calculate_accuracy(predictions, targets):\n",
        "  \"\"\"\n",
        "  Calculate the accuracy of the predictions.\n",
        "\n",
        "  :predictions: the predicted star ratings.\n",
        "  :targets: the ground-truth labels.\n",
        "  \"\"\"\n",
        "\n",
        "  num_correct = (predictions == targets).sum().item()\n",
        "  \n",
        "  return num_correct"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKH3VnHYpo0j"
      },
      "source": [
        "def get_star_predictions(df, model):\n",
        "  \"\"\"\n",
        "  Uses the given pretrained model to predict star ratings based on\n",
        "  reviews. Returns an array with the original reviews and their star\n",
        "  predictions, as well as the accuracy of the predictions.\n",
        "\n",
        "  :df: DataFrame containing reviews and their labels.\n",
        "  :model: loaded pretrained BERT model.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define the prediction parameters.\n",
        "  MAX_LEN = 256\n",
        "  TEST_BATCH_SIZE = 16\n",
        "  NUM_WORKERS = 4\n",
        "\n",
        "  test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "              'shuffle': False,\n",
        "              'num_workers': NUM_WORKERS}\n",
        "\n",
        "  # Define the dataset and dataloader.\n",
        "  dataset = ReviewsDataset(df, MAX_LEN)\n",
        "  data_loader = DataLoader(dataset, **test_params)\n",
        "\n",
        "  num_batches = 0\n",
        "  num_examples = 0\n",
        "  num_correct = 0\n",
        "  total_examples = len(df)\n",
        "  predictions = np.zeros([total_examples, 2], dtype=object)\n",
        "\n",
        "  for batch, data in enumerate(data_loader):\n",
        "    \n",
        "    # Get the tokenization values.\n",
        "    input_ids = data['input_ids'].to(device)\n",
        "    mask = data['attn_mask'].to(device)\n",
        "    labels = data['label'].to(device)\n",
        "\n",
        "    # Make predictions with the trained model\n",
        "    outputs = model(input_ids, mask)\n",
        "    \n",
        "    # Get the star ratings.\n",
        "    big_val, big_idx = torch.max(outputs[0].data, dim=1)\n",
        "    star_predictions = (big_idx + 1).cpu().numpy()\n",
        "    reviews = df['comment'].values[num_examples:num_examples + labels.size(0)]\n",
        "    batch = np.vstack((reviews, star_predictions)).T\n",
        "\n",
        "    # Update the output.\n",
        "    predictions[num_examples:num_examples + labels.size(0)] = batch\n",
        "\n",
        "    num_correct += calculate_accuracy(big_idx, labels)\n",
        "\n",
        "    num_batches += 1\n",
        "    num_examples += labels.size(0)\n",
        "\n",
        "    if num_batches % 10 == 0:\n",
        "      print(\"Batch #{}: {}/{}\".format(num_batches, num_examples, total_examples))\n",
        "\n",
        "  accuracy = (num_correct * 100) / num_examples\n",
        "\n",
        "  print(\"Finished predictions! Accuracy:\", accuracy)\n",
        "\n",
        "  return predictions, accuracy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJyCZCUnifbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fe81bc-e62a-4841-e766-355e69c6f893"
      },
      "source": [
        "# Get the star predictions.\n",
        "predictions, accuracy = get_star_predictions(df, model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch #10: 160/195765\n",
            "Batch #20: 320/195765\n",
            "Batch #30: 480/195765\n",
            "Batch #40: 640/195765\n",
            "Batch #50: 800/195765\n",
            "Batch #60: 960/195765\n",
            "Batch #70: 1120/195765\n",
            "Batch #80: 1280/195765\n",
            "Batch #90: 1440/195765\n",
            "Batch #100: 1600/195765\n",
            "Batch #110: 1760/195765\n",
            "Batch #120: 1920/195765\n",
            "Batch #130: 2080/195765\n",
            "Batch #140: 2240/195765\n",
            "Batch #150: 2400/195765\n",
            "Batch #160: 2560/195765\n",
            "Batch #170: 2720/195765\n",
            "Batch #180: 2880/195765\n",
            "Batch #190: 3040/195765\n",
            "Batch #200: 3200/195765\n",
            "Batch #210: 3360/195765\n",
            "Batch #220: 3520/195765\n",
            "Batch #230: 3680/195765\n",
            "Batch #240: 3840/195765\n",
            "Batch #250: 4000/195765\n",
            "Batch #260: 4160/195765\n",
            "Batch #270: 4320/195765\n",
            "Batch #280: 4480/195765\n",
            "Batch #290: 4640/195765\n",
            "Batch #300: 4800/195765\n",
            "Batch #310: 4960/195765\n",
            "Batch #320: 5120/195765\n",
            "Batch #330: 5280/195765\n",
            "Batch #340: 5440/195765\n",
            "Batch #350: 5600/195765\n",
            "Batch #360: 5760/195765\n",
            "Batch #370: 5920/195765\n",
            "Batch #380: 6080/195765\n",
            "Batch #390: 6240/195765\n",
            "Batch #400: 6400/195765\n",
            "Batch #410: 6560/195765\n",
            "Batch #420: 6720/195765\n",
            "Batch #430: 6880/195765\n",
            "Batch #440: 7040/195765\n",
            "Batch #450: 7200/195765\n",
            "Batch #460: 7360/195765\n",
            "Batch #470: 7520/195765\n",
            "Batch #480: 7680/195765\n",
            "Batch #490: 7840/195765\n",
            "Batch #500: 8000/195765\n",
            "Batch #510: 8160/195765\n",
            "Batch #520: 8320/195765\n",
            "Batch #530: 8480/195765\n",
            "Batch #540: 8640/195765\n",
            "Batch #550: 8800/195765\n",
            "Batch #560: 8960/195765\n",
            "Batch #570: 9120/195765\n",
            "Batch #580: 9280/195765\n",
            "Batch #590: 9440/195765\n",
            "Batch #600: 9600/195765\n",
            "Batch #610: 9760/195765\n",
            "Batch #620: 9920/195765\n",
            "Batch #630: 10080/195765\n",
            "Batch #640: 10240/195765\n",
            "Batch #650: 10400/195765\n",
            "Batch #660: 10560/195765\n",
            "Batch #670: 10720/195765\n",
            "Batch #680: 10880/195765\n",
            "Batch #690: 11040/195765\n",
            "Batch #700: 11200/195765\n",
            "Batch #710: 11360/195765\n",
            "Batch #720: 11520/195765\n",
            "Batch #730: 11680/195765\n",
            "Batch #740: 11840/195765\n",
            "Batch #750: 12000/195765\n",
            "Batch #760: 12160/195765\n",
            "Batch #770: 12320/195765\n",
            "Batch #780: 12480/195765\n",
            "Batch #790: 12640/195765\n",
            "Batch #800: 12800/195765\n",
            "Batch #810: 12960/195765\n",
            "Batch #820: 13120/195765\n",
            "Batch #830: 13280/195765\n",
            "Batch #840: 13440/195765\n",
            "Batch #850: 13600/195765\n",
            "Batch #860: 13760/195765\n",
            "Batch #870: 13920/195765\n",
            "Batch #880: 14080/195765\n",
            "Batch #890: 14240/195765\n",
            "Batch #900: 14400/195765\n",
            "Batch #910: 14560/195765\n",
            "Batch #920: 14720/195765\n",
            "Batch #930: 14880/195765\n",
            "Batch #940: 15040/195765\n",
            "Batch #950: 15200/195765\n",
            "Batch #960: 15360/195765\n",
            "Batch #970: 15520/195765\n",
            "Batch #980: 15680/195765\n",
            "Batch #990: 15840/195765\n",
            "Batch #1000: 16000/195765\n",
            "Batch #1010: 16160/195765\n",
            "Batch #1020: 16320/195765\n",
            "Batch #1030: 16480/195765\n",
            "Batch #1040: 16640/195765\n",
            "Batch #1050: 16800/195765\n",
            "Batch #1060: 16960/195765\n",
            "Batch #1070: 17120/195765\n",
            "Batch #1080: 17280/195765\n",
            "Batch #1090: 17440/195765\n",
            "Batch #1100: 17600/195765\n",
            "Batch #1110: 17760/195765\n",
            "Batch #1120: 17920/195765\n",
            "Batch #1130: 18080/195765\n",
            "Batch #1140: 18240/195765\n",
            "Batch #1150: 18400/195765\n",
            "Batch #1160: 18560/195765\n",
            "Batch #1170: 18720/195765\n",
            "Batch #1180: 18880/195765\n",
            "Batch #1190: 19040/195765\n",
            "Batch #1200: 19200/195765\n",
            "Batch #1210: 19360/195765\n",
            "Batch #1220: 19520/195765\n",
            "Batch #1230: 19680/195765\n",
            "Batch #1240: 19840/195765\n",
            "Batch #1250: 20000/195765\n",
            "Batch #1260: 20160/195765\n",
            "Batch #1270: 20320/195765\n",
            "Batch #1280: 20480/195765\n",
            "Batch #1290: 20640/195765\n",
            "Batch #1300: 20800/195765\n",
            "Batch #1310: 20960/195765\n",
            "Batch #1320: 21120/195765\n",
            "Batch #1330: 21280/195765\n",
            "Batch #1340: 21440/195765\n",
            "Batch #1350: 21600/195765\n",
            "Batch #1360: 21760/195765\n",
            "Batch #1370: 21920/195765\n",
            "Batch #1380: 22080/195765\n",
            "Batch #1390: 22240/195765\n",
            "Batch #1400: 22400/195765\n",
            "Batch #1410: 22560/195765\n",
            "Batch #1420: 22720/195765\n",
            "Batch #1430: 22880/195765\n",
            "Batch #1440: 23040/195765\n",
            "Batch #1450: 23200/195765\n",
            "Batch #1460: 23360/195765\n",
            "Batch #1470: 23520/195765\n",
            "Batch #1480: 23680/195765\n",
            "Batch #1490: 23840/195765\n",
            "Batch #1500: 24000/195765\n",
            "Batch #1510: 24160/195765\n",
            "Batch #1520: 24320/195765\n",
            "Batch #1530: 24480/195765\n",
            "Batch #1540: 24640/195765\n",
            "Batch #1550: 24800/195765\n",
            "Batch #1560: 24960/195765\n",
            "Batch #1570: 25120/195765\n",
            "Batch #1580: 25280/195765\n",
            "Batch #1590: 25440/195765\n",
            "Batch #1600: 25600/195765\n",
            "Batch #1610: 25760/195765\n",
            "Batch #1620: 25920/195765\n",
            "Batch #1630: 26080/195765\n",
            "Batch #1640: 26240/195765\n",
            "Batch #1650: 26400/195765\n",
            "Batch #1660: 26560/195765\n",
            "Batch #1670: 26720/195765\n",
            "Batch #1680: 26880/195765\n",
            "Batch #1690: 27040/195765\n",
            "Batch #1700: 27200/195765\n",
            "Batch #1710: 27360/195765\n",
            "Batch #1720: 27520/195765\n",
            "Batch #1730: 27680/195765\n",
            "Batch #1740: 27840/195765\n",
            "Batch #1750: 28000/195765\n",
            "Batch #1760: 28160/195765\n",
            "Batch #1770: 28320/195765\n",
            "Batch #1780: 28480/195765\n",
            "Batch #1790: 28640/195765\n",
            "Batch #1800: 28800/195765\n",
            "Batch #1810: 28960/195765\n",
            "Batch #1820: 29120/195765\n",
            "Batch #1830: 29280/195765\n",
            "Batch #1840: 29440/195765\n",
            "Batch #1850: 29600/195765\n",
            "Batch #1860: 29760/195765\n",
            "Batch #1870: 29920/195765\n",
            "Batch #1880: 30080/195765\n",
            "Batch #1890: 30240/195765\n",
            "Batch #1900: 30400/195765\n",
            "Batch #1910: 30560/195765\n",
            "Batch #1920: 30720/195765\n",
            "Batch #1930: 30880/195765\n",
            "Batch #1940: 31040/195765\n",
            "Batch #1950: 31200/195765\n",
            "Batch #1960: 31360/195765\n",
            "Batch #1970: 31520/195765\n",
            "Batch #1980: 31680/195765\n",
            "Batch #1990: 31840/195765\n",
            "Batch #2000: 32000/195765\n",
            "Batch #2010: 32160/195765\n",
            "Batch #2020: 32320/195765\n",
            "Batch #2030: 32480/195765\n",
            "Batch #2040: 32640/195765\n",
            "Batch #2050: 32800/195765\n",
            "Batch #2060: 32960/195765\n",
            "Batch #2070: 33120/195765\n",
            "Batch #2080: 33280/195765\n",
            "Batch #2090: 33440/195765\n",
            "Batch #2100: 33600/195765\n",
            "Batch #2110: 33760/195765\n",
            "Batch #2120: 33920/195765\n",
            "Batch #2130: 34080/195765\n",
            "Batch #2140: 34240/195765\n",
            "Batch #2150: 34400/195765\n",
            "Batch #2160: 34560/195765\n",
            "Batch #2170: 34720/195765\n",
            "Batch #2180: 34880/195765\n",
            "Batch #2190: 35040/195765\n",
            "Batch #2200: 35200/195765\n",
            "Batch #2210: 35360/195765\n",
            "Batch #2220: 35520/195765\n",
            "Batch #2230: 35680/195765\n",
            "Batch #2240: 35840/195765\n",
            "Batch #2250: 36000/195765\n",
            "Batch #2260: 36160/195765\n",
            "Batch #2270: 36320/195765\n",
            "Batch #2280: 36480/195765\n",
            "Batch #2290: 36640/195765\n",
            "Batch #2300: 36800/195765\n",
            "Batch #2310: 36960/195765\n",
            "Batch #2320: 37120/195765\n",
            "Batch #2330: 37280/195765\n",
            "Batch #2340: 37440/195765\n",
            "Batch #2350: 37600/195765\n",
            "Batch #2360: 37760/195765\n",
            "Batch #2370: 37920/195765\n",
            "Batch #2380: 38080/195765\n",
            "Batch #2390: 38240/195765\n",
            "Batch #2400: 38400/195765\n",
            "Batch #2410: 38560/195765\n",
            "Batch #2420: 38720/195765\n",
            "Batch #2430: 38880/195765\n",
            "Batch #2440: 39040/195765\n",
            "Batch #2450: 39200/195765\n",
            "Batch #2460: 39360/195765\n",
            "Batch #2470: 39520/195765\n",
            "Batch #2480: 39680/195765\n",
            "Batch #2490: 39840/195765\n",
            "Batch #2500: 40000/195765\n",
            "Batch #2510: 40160/195765\n",
            "Batch #2520: 40320/195765\n",
            "Batch #2530: 40480/195765\n",
            "Batch #2540: 40640/195765\n",
            "Batch #2550: 40800/195765\n",
            "Batch #2560: 40960/195765\n",
            "Batch #2570: 41120/195765\n",
            "Batch #2580: 41280/195765\n",
            "Batch #2590: 41440/195765\n",
            "Batch #2600: 41600/195765\n",
            "Batch #2610: 41760/195765\n",
            "Batch #2620: 41920/195765\n",
            "Batch #2630: 42080/195765\n",
            "Batch #2640: 42240/195765\n",
            "Batch #2650: 42400/195765\n",
            "Batch #2660: 42560/195765\n",
            "Batch #2670: 42720/195765\n",
            "Batch #2680: 42880/195765\n",
            "Batch #2690: 43040/195765\n",
            "Batch #2700: 43200/195765\n",
            "Batch #2710: 43360/195765\n",
            "Batch #2720: 43520/195765\n",
            "Batch #2730: 43680/195765\n",
            "Batch #2740: 43840/195765\n",
            "Batch #2750: 44000/195765\n",
            "Batch #2760: 44160/195765\n",
            "Batch #2770: 44320/195765\n",
            "Batch #2780: 44480/195765\n",
            "Batch #2790: 44640/195765\n",
            "Batch #2800: 44800/195765\n",
            "Batch #2810: 44960/195765\n",
            "Batch #2820: 45120/195765\n",
            "Batch #2830: 45280/195765\n",
            "Batch #2840: 45440/195765\n",
            "Batch #2850: 45600/195765\n",
            "Batch #2860: 45760/195765\n",
            "Batch #2870: 45920/195765\n",
            "Batch #2880: 46080/195765\n",
            "Batch #2890: 46240/195765\n",
            "Batch #2900: 46400/195765\n",
            "Batch #2910: 46560/195765\n",
            "Batch #2920: 46720/195765\n",
            "Batch #2930: 46880/195765\n",
            "Batch #2940: 47040/195765\n",
            "Batch #2950: 47200/195765\n",
            "Batch #2960: 47360/195765\n",
            "Batch #2970: 47520/195765\n",
            "Batch #2980: 47680/195765\n",
            "Batch #2990: 47840/195765\n",
            "Batch #3000: 48000/195765\n",
            "Batch #3010: 48160/195765\n",
            "Batch #3020: 48320/195765\n",
            "Batch #3030: 48480/195765\n",
            "Batch #3040: 48640/195765\n",
            "Batch #3050: 48800/195765\n",
            "Batch #3060: 48960/195765\n",
            "Batch #3070: 49120/195765\n",
            "Batch #3080: 49280/195765\n",
            "Batch #3090: 49440/195765\n",
            "Batch #3100: 49600/195765\n",
            "Batch #3110: 49760/195765\n",
            "Batch #3120: 49920/195765\n",
            "Batch #3130: 50080/195765\n",
            "Batch #3140: 50240/195765\n",
            "Batch #3150: 50400/195765\n",
            "Batch #3160: 50560/195765\n",
            "Batch #3170: 50720/195765\n",
            "Batch #3180: 50880/195765\n",
            "Batch #3190: 51040/195765\n",
            "Batch #3200: 51200/195765\n",
            "Batch #3210: 51360/195765\n",
            "Batch #3220: 51520/195765\n",
            "Batch #3230: 51680/195765\n",
            "Batch #3240: 51840/195765\n",
            "Batch #3250: 52000/195765\n",
            "Batch #3260: 52160/195765\n",
            "Batch #3270: 52320/195765\n",
            "Batch #3280: 52480/195765\n",
            "Batch #3290: 52640/195765\n",
            "Batch #3300: 52800/195765\n",
            "Batch #3310: 52960/195765\n",
            "Batch #3320: 53120/195765\n",
            "Batch #3330: 53280/195765\n",
            "Batch #3340: 53440/195765\n",
            "Batch #3350: 53600/195765\n",
            "Batch #3360: 53760/195765\n",
            "Batch #3370: 53920/195765\n",
            "Batch #3380: 54080/195765\n",
            "Batch #3390: 54240/195765\n",
            "Batch #3400: 54400/195765\n",
            "Batch #3410: 54560/195765\n",
            "Batch #3420: 54720/195765\n",
            "Batch #3430: 54880/195765\n",
            "Batch #3440: 55040/195765\n",
            "Batch #3450: 55200/195765\n",
            "Batch #3460: 55360/195765\n",
            "Batch #3470: 55520/195765\n",
            "Batch #3480: 55680/195765\n",
            "Batch #3490: 55840/195765\n",
            "Batch #3500: 56000/195765\n",
            "Batch #3510: 56160/195765\n",
            "Batch #3520: 56320/195765\n",
            "Batch #3530: 56480/195765\n",
            "Batch #3540: 56640/195765\n",
            "Batch #3550: 56800/195765\n",
            "Batch #3560: 56960/195765\n",
            "Batch #3570: 57120/195765\n",
            "Batch #3580: 57280/195765\n",
            "Batch #3590: 57440/195765\n",
            "Batch #3600: 57600/195765\n",
            "Batch #3610: 57760/195765\n",
            "Batch #3620: 57920/195765\n",
            "Batch #3630: 58080/195765\n",
            "Batch #3640: 58240/195765\n",
            "Batch #3650: 58400/195765\n",
            "Batch #3660: 58560/195765\n",
            "Batch #3670: 58720/195765\n",
            "Batch #3680: 58880/195765\n",
            "Batch #3690: 59040/195765\n",
            "Batch #3700: 59200/195765\n",
            "Batch #3710: 59360/195765\n",
            "Batch #3720: 59520/195765\n",
            "Batch #3730: 59680/195765\n",
            "Batch #3740: 59840/195765\n",
            "Batch #3750: 60000/195765\n",
            "Batch #3760: 60160/195765\n",
            "Batch #3770: 60320/195765\n",
            "Batch #3780: 60480/195765\n",
            "Batch #3790: 60640/195765\n",
            "Batch #3800: 60800/195765\n",
            "Batch #3810: 60960/195765\n",
            "Batch #3820: 61120/195765\n",
            "Batch #3830: 61280/195765\n",
            "Batch #3840: 61440/195765\n",
            "Batch #3850: 61600/195765\n",
            "Batch #3860: 61760/195765\n",
            "Batch #3870: 61920/195765\n",
            "Batch #3880: 62080/195765\n",
            "Batch #3890: 62240/195765\n",
            "Batch #3900: 62400/195765\n",
            "Batch #3910: 62560/195765\n",
            "Batch #3920: 62720/195765\n",
            "Batch #3930: 62880/195765\n",
            "Batch #3940: 63040/195765\n",
            "Batch #3950: 63200/195765\n",
            "Batch #3960: 63360/195765\n",
            "Batch #3970: 63520/195765\n",
            "Batch #3980: 63680/195765\n",
            "Batch #3990: 63840/195765\n",
            "Batch #4000: 64000/195765\n",
            "Batch #4010: 64160/195765\n",
            "Batch #4020: 64320/195765\n",
            "Batch #4030: 64480/195765\n",
            "Batch #4040: 64640/195765\n",
            "Batch #4050: 64800/195765\n",
            "Batch #4060: 64960/195765\n",
            "Batch #4070: 65120/195765\n",
            "Batch #4080: 65280/195765\n",
            "Batch #4090: 65440/195765\n",
            "Batch #4100: 65600/195765\n",
            "Batch #4110: 65760/195765\n",
            "Batch #4120: 65920/195765\n",
            "Batch #4130: 66080/195765\n",
            "Batch #4140: 66240/195765\n",
            "Batch #4150: 66400/195765\n",
            "Batch #4160: 66560/195765\n",
            "Batch #4170: 66720/195765\n",
            "Batch #4180: 66880/195765\n",
            "Batch #4190: 67040/195765\n",
            "Batch #4200: 67200/195765\n",
            "Batch #4210: 67360/195765\n",
            "Batch #4220: 67520/195765\n",
            "Batch #4230: 67680/195765\n",
            "Batch #4240: 67840/195765\n",
            "Batch #4250: 68000/195765\n",
            "Batch #4260: 68160/195765\n",
            "Batch #4270: 68320/195765\n",
            "Batch #4280: 68480/195765\n",
            "Batch #4290: 68640/195765\n",
            "Batch #4300: 68800/195765\n",
            "Batch #4310: 68960/195765\n",
            "Batch #4320: 69120/195765\n",
            "Batch #4330: 69280/195765\n",
            "Batch #4340: 69440/195765\n",
            "Batch #4350: 69600/195765\n",
            "Batch #4360: 69760/195765\n",
            "Batch #4370: 69920/195765\n",
            "Batch #4380: 70080/195765\n",
            "Batch #4390: 70240/195765\n",
            "Batch #4400: 70400/195765\n",
            "Batch #4410: 70560/195765\n",
            "Batch #4420: 70720/195765\n",
            "Batch #4430: 70880/195765\n",
            "Batch #4440: 71040/195765\n",
            "Batch #4450: 71200/195765\n",
            "Batch #4460: 71360/195765\n",
            "Batch #4470: 71520/195765\n",
            "Batch #4480: 71680/195765\n",
            "Batch #4490: 71840/195765\n",
            "Batch #4500: 72000/195765\n",
            "Batch #4510: 72160/195765\n",
            "Batch #4520: 72320/195765\n",
            "Batch #4530: 72480/195765\n",
            "Batch #4540: 72640/195765\n",
            "Batch #4550: 72800/195765\n",
            "Batch #4560: 72960/195765\n",
            "Batch #4570: 73120/195765\n",
            "Batch #4580: 73280/195765\n",
            "Batch #4590: 73440/195765\n",
            "Batch #4600: 73600/195765\n",
            "Batch #4610: 73760/195765\n",
            "Batch #4620: 73920/195765\n",
            "Batch #4630: 74080/195765\n",
            "Batch #4640: 74240/195765\n",
            "Batch #4650: 74400/195765\n",
            "Batch #4660: 74560/195765\n",
            "Batch #4670: 74720/195765\n",
            "Batch #4680: 74880/195765\n",
            "Batch #4690: 75040/195765\n",
            "Batch #4700: 75200/195765\n",
            "Batch #4710: 75360/195765\n",
            "Batch #4720: 75520/195765\n",
            "Batch #4730: 75680/195765\n",
            "Batch #4740: 75840/195765\n",
            "Batch #4750: 76000/195765\n",
            "Batch #4760: 76160/195765\n",
            "Batch #4770: 76320/195765\n",
            "Batch #4780: 76480/195765\n",
            "Batch #4790: 76640/195765\n",
            "Batch #4800: 76800/195765\n",
            "Batch #4810: 76960/195765\n",
            "Batch #4820: 77120/195765\n",
            "Batch #4830: 77280/195765\n",
            "Batch #4840: 77440/195765\n",
            "Batch #4850: 77600/195765\n",
            "Batch #4860: 77760/195765\n",
            "Batch #4870: 77920/195765\n",
            "Batch #4880: 78080/195765\n",
            "Batch #4890: 78240/195765\n",
            "Batch #4900: 78400/195765\n",
            "Batch #4910: 78560/195765\n",
            "Batch #4920: 78720/195765\n",
            "Batch #4930: 78880/195765\n",
            "Batch #4940: 79040/195765\n",
            "Batch #4950: 79200/195765\n",
            "Batch #4960: 79360/195765\n",
            "Batch #4970: 79520/195765\n",
            "Batch #4980: 79680/195765\n",
            "Batch #4990: 79840/195765\n",
            "Batch #5000: 80000/195765\n",
            "Batch #5010: 80160/195765\n",
            "Batch #5020: 80320/195765\n",
            "Batch #5030: 80480/195765\n",
            "Batch #5040: 80640/195765\n",
            "Batch #5050: 80800/195765\n",
            "Batch #5060: 80960/195765\n",
            "Batch #5070: 81120/195765\n",
            "Batch #5080: 81280/195765\n",
            "Batch #5090: 81440/195765\n",
            "Batch #5100: 81600/195765\n",
            "Batch #5110: 81760/195765\n",
            "Batch #5120: 81920/195765\n",
            "Batch #5130: 82080/195765\n",
            "Batch #5140: 82240/195765\n",
            "Batch #5150: 82400/195765\n",
            "Batch #5160: 82560/195765\n",
            "Batch #5170: 82720/195765\n",
            "Batch #5180: 82880/195765\n",
            "Batch #5190: 83040/195765\n",
            "Batch #5200: 83200/195765\n",
            "Batch #5210: 83360/195765\n",
            "Batch #5220: 83520/195765\n",
            "Batch #5230: 83680/195765\n",
            "Batch #5240: 83840/195765\n",
            "Batch #5250: 84000/195765\n",
            "Batch #5260: 84160/195765\n",
            "Batch #5270: 84320/195765\n",
            "Batch #5280: 84480/195765\n",
            "Batch #5290: 84640/195765\n",
            "Batch #5300: 84800/195765\n",
            "Batch #5310: 84960/195765\n",
            "Batch #5320: 85120/195765\n",
            "Batch #5330: 85280/195765\n",
            "Batch #5340: 85440/195765\n",
            "Batch #5350: 85600/195765\n",
            "Batch #5360: 85760/195765\n",
            "Batch #5370: 85920/195765\n",
            "Batch #5380: 86080/195765\n",
            "Batch #5390: 86240/195765\n",
            "Batch #5400: 86400/195765\n",
            "Batch #5410: 86560/195765\n",
            "Batch #5420: 86720/195765\n",
            "Batch #5430: 86880/195765\n",
            "Batch #5440: 87040/195765\n",
            "Batch #5450: 87200/195765\n",
            "Batch #5460: 87360/195765\n",
            "Batch #5470: 87520/195765\n",
            "Batch #5480: 87680/195765\n",
            "Batch #5490: 87840/195765\n",
            "Batch #5500: 88000/195765\n",
            "Batch #5510: 88160/195765\n",
            "Batch #5520: 88320/195765\n",
            "Batch #5530: 88480/195765\n",
            "Batch #5540: 88640/195765\n",
            "Batch #5550: 88800/195765\n",
            "Batch #5560: 88960/195765\n",
            "Batch #5570: 89120/195765\n",
            "Batch #5580: 89280/195765\n",
            "Batch #5590: 89440/195765\n",
            "Batch #5600: 89600/195765\n",
            "Batch #5610: 89760/195765\n",
            "Batch #5620: 89920/195765\n",
            "Batch #5630: 90080/195765\n",
            "Batch #5640: 90240/195765\n",
            "Batch #5650: 90400/195765\n",
            "Batch #5660: 90560/195765\n",
            "Batch #5670: 90720/195765\n",
            "Batch #5680: 90880/195765\n",
            "Batch #5690: 91040/195765\n",
            "Batch #5700: 91200/195765\n",
            "Batch #5710: 91360/195765\n",
            "Batch #5720: 91520/195765\n",
            "Batch #5730: 91680/195765\n",
            "Batch #5740: 91840/195765\n",
            "Batch #5750: 92000/195765\n",
            "Batch #5760: 92160/195765\n",
            "Batch #5770: 92320/195765\n",
            "Batch #5780: 92480/195765\n",
            "Batch #5790: 92640/195765\n",
            "Batch #5800: 92800/195765\n",
            "Batch #5810: 92960/195765\n",
            "Batch #5820: 93120/195765\n",
            "Batch #5830: 93280/195765\n",
            "Batch #5840: 93440/195765\n",
            "Batch #5850: 93600/195765\n",
            "Batch #5860: 93760/195765\n",
            "Batch #5870: 93920/195765\n",
            "Batch #5880: 94080/195765\n",
            "Batch #5890: 94240/195765\n",
            "Batch #5900: 94400/195765\n",
            "Batch #5910: 94560/195765\n",
            "Batch #5920: 94720/195765\n",
            "Batch #5930: 94880/195765\n",
            "Batch #5940: 95040/195765\n",
            "Batch #5950: 95200/195765\n",
            "Batch #5960: 95360/195765\n",
            "Batch #5970: 95520/195765\n",
            "Batch #5980: 95680/195765\n",
            "Batch #5990: 95840/195765\n",
            "Batch #6000: 96000/195765\n",
            "Batch #6010: 96160/195765\n",
            "Batch #6020: 96320/195765\n",
            "Batch #6030: 96480/195765\n",
            "Batch #6040: 96640/195765\n",
            "Batch #6050: 96800/195765\n",
            "Batch #6060: 96960/195765\n",
            "Batch #6070: 97120/195765\n",
            "Batch #6080: 97280/195765\n",
            "Batch #6090: 97440/195765\n",
            "Batch #6100: 97600/195765\n",
            "Batch #6110: 97760/195765\n",
            "Batch #6120: 97920/195765\n",
            "Batch #6130: 98080/195765\n",
            "Batch #6140: 98240/195765\n",
            "Batch #6150: 98400/195765\n",
            "Batch #6160: 98560/195765\n",
            "Batch #6170: 98720/195765\n",
            "Batch #6180: 98880/195765\n",
            "Batch #6190: 99040/195765\n",
            "Batch #6200: 99200/195765\n",
            "Batch #6210: 99360/195765\n",
            "Batch #6220: 99520/195765\n",
            "Batch #6230: 99680/195765\n",
            "Batch #6240: 99840/195765\n",
            "Batch #6250: 100000/195765\n",
            "Batch #6260: 100160/195765\n",
            "Batch #6270: 100320/195765\n",
            "Batch #6280: 100480/195765\n",
            "Batch #6290: 100640/195765\n",
            "Batch #6300: 100800/195765\n",
            "Batch #6310: 100960/195765\n",
            "Batch #6320: 101120/195765\n",
            "Batch #6330: 101280/195765\n",
            "Batch #6340: 101440/195765\n",
            "Batch #6350: 101600/195765\n",
            "Batch #6360: 101760/195765\n",
            "Batch #6370: 101920/195765\n",
            "Batch #6380: 102080/195765\n",
            "Batch #6390: 102240/195765\n",
            "Batch #6400: 102400/195765\n",
            "Batch #6410: 102560/195765\n",
            "Batch #6420: 102720/195765\n",
            "Batch #6430: 102880/195765\n",
            "Batch #6440: 103040/195765\n",
            "Batch #6450: 103200/195765\n",
            "Batch #6460: 103360/195765\n",
            "Batch #6470: 103520/195765\n",
            "Batch #6480: 103680/195765\n",
            "Batch #6490: 103840/195765\n",
            "Batch #6500: 104000/195765\n",
            "Batch #6510: 104160/195765\n",
            "Batch #6520: 104320/195765\n",
            "Batch #6530: 104480/195765\n",
            "Batch #6540: 104640/195765\n",
            "Batch #6550: 104800/195765\n",
            "Batch #6560: 104960/195765\n",
            "Batch #6570: 105120/195765\n",
            "Batch #6580: 105280/195765\n",
            "Batch #6590: 105440/195765\n",
            "Batch #6600: 105600/195765\n",
            "Batch #6610: 105760/195765\n",
            "Batch #6620: 105920/195765\n",
            "Batch #6630: 106080/195765\n",
            "Batch #6640: 106240/195765\n",
            "Batch #6650: 106400/195765\n",
            "Batch #6660: 106560/195765\n",
            "Batch #6670: 106720/195765\n",
            "Batch #6680: 106880/195765\n",
            "Batch #6690: 107040/195765\n",
            "Batch #6700: 107200/195765\n",
            "Batch #6710: 107360/195765\n",
            "Batch #6720: 107520/195765\n",
            "Batch #6730: 107680/195765\n",
            "Batch #6740: 107840/195765\n",
            "Batch #6750: 108000/195765\n",
            "Batch #6760: 108160/195765\n",
            "Batch #6770: 108320/195765\n",
            "Batch #6780: 108480/195765\n",
            "Batch #6790: 108640/195765\n",
            "Batch #6800: 108800/195765\n",
            "Batch #6810: 108960/195765\n",
            "Batch #6820: 109120/195765\n",
            "Batch #6830: 109280/195765\n",
            "Batch #6840: 109440/195765\n",
            "Batch #6850: 109600/195765\n",
            "Batch #6860: 109760/195765\n",
            "Batch #6870: 109920/195765\n",
            "Batch #6880: 110080/195765\n",
            "Batch #6890: 110240/195765\n",
            "Batch #6900: 110400/195765\n",
            "Batch #6910: 110560/195765\n",
            "Batch #6920: 110720/195765\n",
            "Batch #6930: 110880/195765\n",
            "Batch #6940: 111040/195765\n",
            "Batch #6950: 111200/195765\n",
            "Batch #6960: 111360/195765\n",
            "Batch #6970: 111520/195765\n",
            "Batch #6980: 111680/195765\n",
            "Batch #6990: 111840/195765\n",
            "Batch #7000: 112000/195765\n",
            "Batch #7010: 112160/195765\n",
            "Batch #7020: 112320/195765\n",
            "Batch #7030: 112480/195765\n",
            "Batch #7040: 112640/195765\n",
            "Batch #7050: 112800/195765\n",
            "Batch #7060: 112960/195765\n",
            "Batch #7070: 113120/195765\n",
            "Batch #7080: 113280/195765\n",
            "Batch #7090: 113440/195765\n",
            "Batch #7100: 113600/195765\n",
            "Batch #7110: 113760/195765\n",
            "Batch #7120: 113920/195765\n",
            "Batch #7130: 114080/195765\n",
            "Batch #7140: 114240/195765\n",
            "Batch #7150: 114400/195765\n",
            "Batch #7160: 114560/195765\n",
            "Batch #7170: 114720/195765\n",
            "Batch #7180: 114880/195765\n",
            "Batch #7190: 115040/195765\n",
            "Batch #7200: 115200/195765\n",
            "Batch #7210: 115360/195765\n",
            "Batch #7220: 115520/195765\n",
            "Batch #7230: 115680/195765\n",
            "Batch #7240: 115840/195765\n",
            "Batch #7250: 116000/195765\n",
            "Batch #7260: 116160/195765\n",
            "Batch #7270: 116320/195765\n",
            "Batch #7280: 116480/195765\n",
            "Batch #7290: 116640/195765\n",
            "Batch #7300: 116800/195765\n",
            "Batch #7310: 116960/195765\n",
            "Batch #7320: 117120/195765\n",
            "Batch #7330: 117280/195765\n",
            "Batch #7340: 117440/195765\n",
            "Batch #7350: 117600/195765\n",
            "Batch #7360: 117760/195765\n",
            "Batch #7370: 117920/195765\n",
            "Batch #7380: 118080/195765\n",
            "Batch #7390: 118240/195765\n",
            "Batch #7400: 118400/195765\n",
            "Batch #7410: 118560/195765\n",
            "Batch #7420: 118720/195765\n",
            "Batch #7430: 118880/195765\n",
            "Batch #7440: 119040/195765\n",
            "Batch #7450: 119200/195765\n",
            "Batch #7460: 119360/195765\n",
            "Batch #7470: 119520/195765\n",
            "Batch #7480: 119680/195765\n",
            "Batch #7490: 119840/195765\n",
            "Batch #7500: 120000/195765\n",
            "Batch #7510: 120160/195765\n",
            "Batch #7520: 120320/195765\n",
            "Batch #7530: 120480/195765\n",
            "Batch #7540: 120640/195765\n",
            "Batch #7550: 120800/195765\n",
            "Batch #7560: 120960/195765\n",
            "Batch #7570: 121120/195765\n",
            "Batch #7580: 121280/195765\n",
            "Batch #7590: 121440/195765\n",
            "Batch #7600: 121600/195765\n",
            "Batch #7610: 121760/195765\n",
            "Batch #7620: 121920/195765\n",
            "Batch #7630: 122080/195765\n",
            "Batch #7640: 122240/195765\n",
            "Batch #7650: 122400/195765\n",
            "Batch #7660: 122560/195765\n",
            "Batch #7670: 122720/195765\n",
            "Batch #7680: 122880/195765\n",
            "Batch #7690: 123040/195765\n",
            "Batch #7700: 123200/195765\n",
            "Batch #7710: 123360/195765\n",
            "Batch #7720: 123520/195765\n",
            "Batch #7730: 123680/195765\n",
            "Batch #7740: 123840/195765\n",
            "Batch #7750: 124000/195765\n",
            "Batch #7760: 124160/195765\n",
            "Batch #7770: 124320/195765\n",
            "Batch #7780: 124480/195765\n",
            "Batch #7790: 124640/195765\n",
            "Batch #7800: 124800/195765\n",
            "Batch #7810: 124960/195765\n",
            "Batch #7820: 125120/195765\n",
            "Batch #7830: 125280/195765\n",
            "Batch #7840: 125440/195765\n",
            "Batch #7850: 125600/195765\n",
            "Batch #7860: 125760/195765\n",
            "Batch #7870: 125920/195765\n",
            "Batch #7880: 126080/195765\n",
            "Batch #7890: 126240/195765\n",
            "Batch #7900: 126400/195765\n",
            "Batch #7910: 126560/195765\n",
            "Batch #7920: 126720/195765\n",
            "Batch #7930: 126880/195765\n",
            "Batch #7940: 127040/195765\n",
            "Batch #7950: 127200/195765\n",
            "Batch #7960: 127360/195765\n",
            "Batch #7970: 127520/195765\n",
            "Batch #7980: 127680/195765\n",
            "Batch #7990: 127840/195765\n",
            "Batch #8000: 128000/195765\n",
            "Batch #8010: 128160/195765\n",
            "Batch #8020: 128320/195765\n",
            "Batch #8030: 128480/195765\n",
            "Batch #8040: 128640/195765\n",
            "Batch #8050: 128800/195765\n",
            "Batch #8060: 128960/195765\n",
            "Batch #8070: 129120/195765\n",
            "Batch #8080: 129280/195765\n",
            "Batch #8090: 129440/195765\n",
            "Batch #8100: 129600/195765\n",
            "Batch #8110: 129760/195765\n",
            "Batch #8120: 129920/195765\n",
            "Batch #8130: 130080/195765\n",
            "Batch #8140: 130240/195765\n",
            "Batch #8150: 130400/195765\n",
            "Batch #8160: 130560/195765\n",
            "Batch #8170: 130720/195765\n",
            "Batch #8180: 130880/195765\n",
            "Batch #8190: 131040/195765\n",
            "Batch #8200: 131200/195765\n",
            "Batch #8210: 131360/195765\n",
            "Batch #8220: 131520/195765\n",
            "Batch #8230: 131680/195765\n",
            "Batch #8240: 131840/195765\n",
            "Batch #8250: 132000/195765\n",
            "Batch #8260: 132160/195765\n",
            "Batch #8270: 132320/195765\n",
            "Batch #8280: 132480/195765\n",
            "Batch #8290: 132640/195765\n",
            "Batch #8300: 132800/195765\n",
            "Batch #8310: 132960/195765\n",
            "Batch #8320: 133120/195765\n",
            "Batch #8330: 133280/195765\n",
            "Batch #8340: 133440/195765\n",
            "Batch #8350: 133600/195765\n",
            "Batch #8360: 133760/195765\n",
            "Batch #8370: 133920/195765\n",
            "Batch #8380: 134080/195765\n",
            "Batch #8390: 134240/195765\n",
            "Batch #8400: 134400/195765\n",
            "Batch #8410: 134560/195765\n",
            "Batch #8420: 134720/195765\n",
            "Batch #8430: 134880/195765\n",
            "Batch #8440: 135040/195765\n",
            "Batch #8450: 135200/195765\n",
            "Batch #8460: 135360/195765\n",
            "Batch #8470: 135520/195765\n",
            "Batch #8480: 135680/195765\n",
            "Batch #8490: 135840/195765\n",
            "Batch #8500: 136000/195765\n",
            "Batch #8510: 136160/195765\n",
            "Batch #8520: 136320/195765\n",
            "Batch #8530: 136480/195765\n",
            "Batch #8540: 136640/195765\n",
            "Batch #8550: 136800/195765\n",
            "Batch #8560: 136960/195765\n",
            "Batch #8570: 137120/195765\n",
            "Batch #8580: 137280/195765\n",
            "Batch #8590: 137440/195765\n",
            "Batch #8600: 137600/195765\n",
            "Batch #8610: 137760/195765\n",
            "Batch #8620: 137920/195765\n",
            "Batch #8630: 138080/195765\n",
            "Batch #8640: 138240/195765\n",
            "Batch #8650: 138400/195765\n",
            "Batch #8660: 138560/195765\n",
            "Batch #8670: 138720/195765\n",
            "Batch #8680: 138880/195765\n",
            "Batch #8690: 139040/195765\n",
            "Batch #8700: 139200/195765\n",
            "Batch #8710: 139360/195765\n",
            "Batch #8720: 139520/195765\n",
            "Batch #8730: 139680/195765\n",
            "Batch #8740: 139840/195765\n",
            "Batch #8750: 140000/195765\n",
            "Batch #8760: 140160/195765\n",
            "Batch #8770: 140320/195765\n",
            "Batch #8780: 140480/195765\n",
            "Batch #8790: 140640/195765\n",
            "Batch #8800: 140800/195765\n",
            "Batch #8810: 140960/195765\n",
            "Batch #8820: 141120/195765\n",
            "Batch #8830: 141280/195765\n",
            "Batch #8840: 141440/195765\n",
            "Batch #8850: 141600/195765\n",
            "Batch #8860: 141760/195765\n",
            "Batch #8870: 141920/195765\n",
            "Batch #8880: 142080/195765\n",
            "Batch #8890: 142240/195765\n",
            "Batch #8900: 142400/195765\n",
            "Batch #8910: 142560/195765\n",
            "Batch #8920: 142720/195765\n",
            "Batch #8930: 142880/195765\n",
            "Batch #8940: 143040/195765\n",
            "Batch #8950: 143200/195765\n",
            "Batch #8960: 143360/195765\n",
            "Batch #8970: 143520/195765\n",
            "Batch #8980: 143680/195765\n",
            "Batch #8990: 143840/195765\n",
            "Batch #9000: 144000/195765\n",
            "Batch #9010: 144160/195765\n",
            "Batch #9020: 144320/195765\n",
            "Batch #9030: 144480/195765\n",
            "Batch #9040: 144640/195765\n",
            "Batch #9050: 144800/195765\n",
            "Batch #9060: 144960/195765\n",
            "Batch #9070: 145120/195765\n",
            "Batch #9080: 145280/195765\n",
            "Batch #9090: 145440/195765\n",
            "Batch #9100: 145600/195765\n",
            "Batch #9110: 145760/195765\n",
            "Batch #9120: 145920/195765\n",
            "Batch #9130: 146080/195765\n",
            "Batch #9140: 146240/195765\n",
            "Batch #9150: 146400/195765\n",
            "Batch #9160: 146560/195765\n",
            "Batch #9170: 146720/195765\n",
            "Batch #9180: 146880/195765\n",
            "Batch #9190: 147040/195765\n",
            "Batch #9200: 147200/195765\n",
            "Batch #9210: 147360/195765\n",
            "Batch #9220: 147520/195765\n",
            "Batch #9230: 147680/195765\n",
            "Batch #9240: 147840/195765\n",
            "Batch #9250: 148000/195765\n",
            "Batch #9260: 148160/195765\n",
            "Batch #9270: 148320/195765\n",
            "Batch #9280: 148480/195765\n",
            "Batch #9290: 148640/195765\n",
            "Batch #9300: 148800/195765\n",
            "Batch #9310: 148960/195765\n",
            "Batch #9320: 149120/195765\n",
            "Batch #9330: 149280/195765\n",
            "Batch #9340: 149440/195765\n",
            "Batch #9350: 149600/195765\n",
            "Batch #9360: 149760/195765\n",
            "Batch #9370: 149920/195765\n",
            "Batch #9380: 150080/195765\n",
            "Batch #9390: 150240/195765\n",
            "Batch #9400: 150400/195765\n",
            "Batch #9410: 150560/195765\n",
            "Batch #9420: 150720/195765\n",
            "Batch #9430: 150880/195765\n",
            "Batch #9440: 151040/195765\n",
            "Batch #9450: 151200/195765\n",
            "Batch #9460: 151360/195765\n",
            "Batch #9470: 151520/195765\n",
            "Batch #9480: 151680/195765\n",
            "Batch #9490: 151840/195765\n",
            "Batch #9500: 152000/195765\n",
            "Batch #9510: 152160/195765\n",
            "Batch #9520: 152320/195765\n",
            "Batch #9530: 152480/195765\n",
            "Batch #9540: 152640/195765\n",
            "Batch #9550: 152800/195765\n",
            "Batch #9560: 152960/195765\n",
            "Batch #9570: 153120/195765\n",
            "Batch #9580: 153280/195765\n",
            "Batch #9590: 153440/195765\n",
            "Batch #9600: 153600/195765\n",
            "Batch #9610: 153760/195765\n",
            "Batch #9620: 153920/195765\n",
            "Batch #9630: 154080/195765\n",
            "Batch #9640: 154240/195765\n",
            "Batch #9650: 154400/195765\n",
            "Batch #9660: 154560/195765\n",
            "Batch #9670: 154720/195765\n",
            "Batch #9680: 154880/195765\n",
            "Batch #9690: 155040/195765\n",
            "Batch #9700: 155200/195765\n",
            "Batch #9710: 155360/195765\n",
            "Batch #9720: 155520/195765\n",
            "Batch #9730: 155680/195765\n",
            "Batch #9740: 155840/195765\n",
            "Batch #9750: 156000/195765\n",
            "Batch #9760: 156160/195765\n",
            "Batch #9770: 156320/195765\n",
            "Batch #9780: 156480/195765\n",
            "Batch #9790: 156640/195765\n",
            "Batch #9800: 156800/195765\n",
            "Batch #9810: 156960/195765\n",
            "Batch #9820: 157120/195765\n",
            "Batch #9830: 157280/195765\n",
            "Batch #9840: 157440/195765\n",
            "Batch #9850: 157600/195765\n",
            "Batch #9860: 157760/195765\n",
            "Batch #9870: 157920/195765\n",
            "Batch #9880: 158080/195765\n",
            "Batch #9890: 158240/195765\n",
            "Batch #9900: 158400/195765\n",
            "Batch #9910: 158560/195765\n",
            "Batch #9920: 158720/195765\n",
            "Batch #9930: 158880/195765\n",
            "Batch #9940: 159040/195765\n",
            "Batch #9950: 159200/195765\n",
            "Batch #9960: 159360/195765\n",
            "Batch #9970: 159520/195765\n",
            "Batch #9980: 159680/195765\n",
            "Batch #9990: 159840/195765\n",
            "Batch #10000: 160000/195765\n",
            "Batch #10010: 160160/195765\n",
            "Batch #10020: 160320/195765\n",
            "Batch #10030: 160480/195765\n",
            "Batch #10040: 160640/195765\n",
            "Batch #10050: 160800/195765\n",
            "Batch #10060: 160960/195765\n",
            "Batch #10070: 161120/195765\n",
            "Batch #10080: 161280/195765\n",
            "Batch #10090: 161440/195765\n",
            "Batch #10100: 161600/195765\n",
            "Batch #10110: 161760/195765\n",
            "Batch #10120: 161920/195765\n",
            "Batch #10130: 162080/195765\n",
            "Batch #10140: 162240/195765\n",
            "Batch #10150: 162400/195765\n",
            "Batch #10160: 162560/195765\n",
            "Batch #10170: 162720/195765\n",
            "Batch #10180: 162880/195765\n",
            "Batch #10190: 163040/195765\n",
            "Batch #10200: 163200/195765\n",
            "Batch #10210: 163360/195765\n",
            "Batch #10220: 163520/195765\n",
            "Batch #10230: 163680/195765\n",
            "Batch #10240: 163840/195765\n",
            "Batch #10250: 164000/195765\n",
            "Batch #10260: 164160/195765\n",
            "Batch #10270: 164320/195765\n",
            "Batch #10280: 164480/195765\n",
            "Batch #10290: 164640/195765\n",
            "Batch #10300: 164800/195765\n",
            "Batch #10310: 164960/195765\n",
            "Batch #10320: 165120/195765\n",
            "Batch #10330: 165280/195765\n",
            "Batch #10340: 165440/195765\n",
            "Batch #10350: 165600/195765\n",
            "Batch #10360: 165760/195765\n",
            "Batch #10370: 165920/195765\n",
            "Batch #10380: 166080/195765\n",
            "Batch #10390: 166240/195765\n",
            "Batch #10400: 166400/195765\n",
            "Batch #10410: 166560/195765\n",
            "Batch #10420: 166720/195765\n",
            "Batch #10430: 166880/195765\n",
            "Batch #10440: 167040/195765\n",
            "Batch #10450: 167200/195765\n",
            "Batch #10460: 167360/195765\n",
            "Batch #10470: 167520/195765\n",
            "Batch #10480: 167680/195765\n",
            "Batch #10490: 167840/195765\n",
            "Batch #10500: 168000/195765\n",
            "Batch #10510: 168160/195765\n",
            "Batch #10520: 168320/195765\n",
            "Batch #10530: 168480/195765\n",
            "Batch #10540: 168640/195765\n",
            "Batch #10550: 168800/195765\n",
            "Batch #10560: 168960/195765\n",
            "Batch #10570: 169120/195765\n",
            "Batch #10580: 169280/195765\n",
            "Batch #10590: 169440/195765\n",
            "Batch #10600: 169600/195765\n",
            "Batch #10610: 169760/195765\n",
            "Batch #10620: 169920/195765\n",
            "Batch #10630: 170080/195765\n",
            "Batch #10640: 170240/195765\n",
            "Batch #10650: 170400/195765\n",
            "Batch #10660: 170560/195765\n",
            "Batch #10670: 170720/195765\n",
            "Batch #10680: 170880/195765\n",
            "Batch #10690: 171040/195765\n",
            "Batch #10700: 171200/195765\n",
            "Batch #10710: 171360/195765\n",
            "Batch #10720: 171520/195765\n",
            "Batch #10730: 171680/195765\n",
            "Batch #10740: 171840/195765\n",
            "Batch #10750: 172000/195765\n",
            "Batch #10760: 172160/195765\n",
            "Batch #10770: 172320/195765\n",
            "Batch #10780: 172480/195765\n",
            "Batch #10790: 172640/195765\n",
            "Batch #10800: 172800/195765\n",
            "Batch #10810: 172960/195765\n",
            "Batch #10820: 173120/195765\n",
            "Batch #10830: 173280/195765\n",
            "Batch #10840: 173440/195765\n",
            "Batch #10850: 173600/195765\n",
            "Batch #10860: 173760/195765\n",
            "Batch #10870: 173920/195765\n",
            "Batch #10880: 174080/195765\n",
            "Batch #10890: 174240/195765\n",
            "Batch #10900: 174400/195765\n",
            "Batch #10910: 174560/195765\n",
            "Batch #10920: 174720/195765\n",
            "Batch #10930: 174880/195765\n",
            "Batch #10940: 175040/195765\n",
            "Batch #10950: 175200/195765\n",
            "Batch #10960: 175360/195765\n",
            "Batch #10970: 175520/195765\n",
            "Batch #10980: 175680/195765\n",
            "Batch #10990: 175840/195765\n",
            "Batch #11000: 176000/195765\n",
            "Batch #11010: 176160/195765\n",
            "Batch #11020: 176320/195765\n",
            "Batch #11030: 176480/195765\n",
            "Batch #11040: 176640/195765\n",
            "Batch #11050: 176800/195765\n",
            "Batch #11060: 176960/195765\n",
            "Batch #11070: 177120/195765\n",
            "Batch #11080: 177280/195765\n",
            "Batch #11090: 177440/195765\n",
            "Batch #11100: 177600/195765\n",
            "Batch #11110: 177760/195765\n",
            "Batch #11120: 177920/195765\n",
            "Batch #11130: 178080/195765\n",
            "Batch #11140: 178240/195765\n",
            "Batch #11150: 178400/195765\n",
            "Batch #11160: 178560/195765\n",
            "Batch #11170: 178720/195765\n",
            "Batch #11180: 178880/195765\n",
            "Batch #11190: 179040/195765\n",
            "Batch #11200: 179200/195765\n",
            "Batch #11210: 179360/195765\n",
            "Batch #11220: 179520/195765\n",
            "Batch #11230: 179680/195765\n",
            "Batch #11240: 179840/195765\n",
            "Batch #11250: 180000/195765\n",
            "Batch #11260: 180160/195765\n",
            "Batch #11270: 180320/195765\n",
            "Batch #11280: 180480/195765\n",
            "Batch #11290: 180640/195765\n",
            "Batch #11300: 180800/195765\n",
            "Batch #11310: 180960/195765\n",
            "Batch #11320: 181120/195765\n",
            "Batch #11330: 181280/195765\n",
            "Batch #11340: 181440/195765\n",
            "Batch #11350: 181600/195765\n",
            "Batch #11360: 181760/195765\n",
            "Batch #11370: 181920/195765\n",
            "Batch #11380: 182080/195765\n",
            "Batch #11390: 182240/195765\n",
            "Batch #11400: 182400/195765\n",
            "Batch #11410: 182560/195765\n",
            "Batch #11420: 182720/195765\n",
            "Batch #11430: 182880/195765\n",
            "Batch #11440: 183040/195765\n",
            "Batch #11450: 183200/195765\n",
            "Batch #11460: 183360/195765\n",
            "Batch #11470: 183520/195765\n",
            "Batch #11480: 183680/195765\n",
            "Batch #11490: 183840/195765\n",
            "Batch #11500: 184000/195765\n",
            "Batch #11510: 184160/195765\n",
            "Batch #11520: 184320/195765\n",
            "Batch #11530: 184480/195765\n",
            "Batch #11540: 184640/195765\n",
            "Batch #11550: 184800/195765\n",
            "Batch #11560: 184960/195765\n",
            "Batch #11570: 185120/195765\n",
            "Batch #11580: 185280/195765\n",
            "Batch #11590: 185440/195765\n",
            "Batch #11600: 185600/195765\n",
            "Batch #11610: 185760/195765\n",
            "Batch #11620: 185920/195765\n",
            "Batch #11630: 186080/195765\n",
            "Batch #11640: 186240/195765\n",
            "Batch #11650: 186400/195765\n",
            "Batch #11660: 186560/195765\n",
            "Batch #11670: 186720/195765\n",
            "Batch #11680: 186880/195765\n",
            "Batch #11690: 187040/195765\n",
            "Batch #11700: 187200/195765\n",
            "Batch #11710: 187360/195765\n",
            "Batch #11720: 187520/195765\n",
            "Batch #11730: 187680/195765\n",
            "Batch #11740: 187840/195765\n",
            "Batch #11750: 188000/195765\n",
            "Batch #11760: 188160/195765\n",
            "Batch #11770: 188320/195765\n",
            "Batch #11780: 188480/195765\n",
            "Batch #11790: 188640/195765\n",
            "Batch #11800: 188800/195765\n",
            "Batch #11810: 188960/195765\n",
            "Batch #11820: 189120/195765\n",
            "Batch #11830: 189280/195765\n",
            "Batch #11840: 189440/195765\n",
            "Batch #11850: 189600/195765\n",
            "Batch #11860: 189760/195765\n",
            "Batch #11870: 189920/195765\n",
            "Batch #11880: 190080/195765\n",
            "Batch #11890: 190240/195765\n",
            "Batch #11900: 190400/195765\n",
            "Batch #11910: 190560/195765\n",
            "Batch #11920: 190720/195765\n",
            "Batch #11930: 190880/195765\n",
            "Batch #11940: 191040/195765\n",
            "Batch #11950: 191200/195765\n",
            "Batch #11960: 191360/195765\n",
            "Batch #11970: 191520/195765\n",
            "Batch #11980: 191680/195765\n",
            "Batch #11990: 191840/195765\n",
            "Batch #12000: 192000/195765\n",
            "Batch #12010: 192160/195765\n",
            "Batch #12020: 192320/195765\n",
            "Batch #12030: 192480/195765\n",
            "Batch #12040: 192640/195765\n",
            "Batch #12050: 192800/195765\n",
            "Batch #12060: 192960/195765\n",
            "Batch #12070: 193120/195765\n",
            "Batch #12080: 193280/195765\n",
            "Batch #12090: 193440/195765\n",
            "Batch #12100: 193600/195765\n",
            "Batch #12110: 193760/195765\n",
            "Batch #12120: 193920/195765\n",
            "Batch #12130: 194080/195765\n",
            "Batch #12140: 194240/195765\n",
            "Batch #12150: 194400/195765\n",
            "Batch #12160: 194560/195765\n",
            "Batch #12170: 194720/195765\n",
            "Batch #12180: 194880/195765\n",
            "Batch #12190: 195040/195765\n",
            "Batch #12200: 195200/195765\n",
            "Batch #12210: 195360/195765\n",
            "Batch #12220: 195520/195765\n",
            "Batch #12230: 195680/195765\n",
            "Finished predictions! Accuracy: 83.52718821035425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-YXnBFZDj5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648747a7-8fed-47c5-a09b-81af6b9eb762"
      },
      "source": [
        "# Show the predictions.\n",
        "print(predictions)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"I could sit here and write all about the specs on this computer, but they are already in the description, and If you are like me... you don't really understand it anyways.So I am going to tell you what I LOVE about this computer and what I use it for. I am a full time college student as well as a single mother who stays busy. I have previously used a HP All In one computer that I bought brand new a year ago and I hate that thing... It is so slow!!! When I first opened this item, I was just hoping that it would be a little faster! What I got instead was an amazing computer that is faster than I could have ever imagined. Now I don't use this thing for much more than amazon reviews, school work, and papers. But this is exactly what I needed.\"\n",
            "  5]\n",
            " ['A very reasonably priced laptop for basic computing needs. The specs that stick out to me for describing this as \"basic needs\" is 4GB of RAM, and 128GB M.2 SSD. Both are at the bare minimum in today\\'s needs. Cell phones now come with those specs( high dollar ones though..).Seems like one of the goals of this laptop is to keep an aesthetically pleasing design. Just 1 3.1 USB port and 2 2.0 USB ports are included, one headphone/mic jack, 1 slim RJ45 port, HDMI, and power input plug. No external battery.'\n",
            "  4]\n",
            " ['This is the best laptop deal you can get, full stop.Touchescreen? Nope. That feature is stupid in a laptop, in my view. As you use the keyboard, incidental touche s of the screen change focus or move to different screens--touchscreen is a gimmick and sucks.'\n",
            "  5]\n",
            " ...\n",
            " ['This is an incredible camera for the money!!  Iâ€™m now using 3 V2â€™s and 2 Pan cameras from Wyze. Iâ€™m very very happy this the camera and there are more coming my way in the future for sure!'\n",
            "  5]\n",
            " ['Great cameras. Purchased some for my mother after installing mine and she loves them as well.'\n",
            "  5]\n",
            " ['Just getting it set up and seem s to work well. I might need to get a tad closer tohave a better visual of door at night time.??!'\n",
            "  5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO99KCBcNEic"
      },
      "source": [
        "## 7. Save the Results in a CSV File\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF7LWXe6NNUD"
      },
      "source": [
        "# Save the predictions as a dataframe.\n",
        "pred_df = pd.DataFrame(data=predictions, columns=['review', 'prediction'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQDsaYwoUISS"
      },
      "source": [
        "# Save the results to a CSV file.\n",
        "pred_df.to_csv('predictions.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}