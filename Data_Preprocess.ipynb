{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Data_Preprocess.ipynb","provenance":[{"file_id":"https://github.com/csbanon/bert-product-rating-predictor/blob/master/.ipynb_checkpoints/Data_Preprocess-checkpoint.ipynb","timestamp":1602083645015}],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"65a96d49d3224cc3ab528950ee4e9c81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a192caaa2f584679b6a7eaca1186ce85","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_23c2f980f1d4474b99aaa81177d3506a","IPY_MODEL_87f578f9436b40febc823c58d31c393d"]}},"a192caaa2f584679b6a7eaca1186ce85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"23c2f980f1d4474b99aaa81177d3506a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1e1e4d2d246e4e8daebc5b1c77badd31","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2dc536845439483e850beb92f836a9a6"}},"87f578f9436b40febc823c58d31c393d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6d664083c5ff4b80882e0356d71f32a1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:01&lt;00:00, 385B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28658b1ed31d4fb88532c162d8e9515b"}},"1e1e4d2d246e4e8daebc5b1c77badd31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2dc536845439483e850beb92f836a9a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d664083c5ff4b80882e0356d71f32a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"28658b1ed31d4fb88532c162d8e9515b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4dc8618e4d354fdead0dc77dd5f11c1d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5fb3184400fb40d182fff22e121e02c3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b95fc9f871f94612b9258c1d8bce1da9","IPY_MODEL_25e8d6ab6700456592f5718b787a2293"]}},"5fb3184400fb40d182fff22e121e02c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b95fc9f871f94612b9258c1d8bce1da9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_44fb43dd4482486ab9c255ddda21a536","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3d1e2c59b134b87a0903c7e09ee8ce0"}},"25e8d6ab6700456592f5718b787a2293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4b6e467fa3cb46eaa8216c04589c16de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 630kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97b37eec53b746bd98cce27a3ce9b201"}},"44fb43dd4482486ab9c255ddda21a536":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f3d1e2c59b134b87a0903c7e09ee8ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b6e467fa3cb46eaa8216c04589c16de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"97b37eec53b746bd98cce27a3ce9b201":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"XodI4wEPZXMX","executionInfo":{"status":"ok","timestamp":1602169146421,"user_tz":240,"elapsed":8347,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"01f733fc-ef2b-415a-c3d3-5c60b82380ca","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n","\r\u001b[K     |▎                               | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 5.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 7.1MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 245kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 256kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 276kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 286kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 307kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 317kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 327kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 337kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 348kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 358kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 368kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 378kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 389kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 409kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 419kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 430kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 440kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 450kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 460kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 471kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 481kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 491kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 501kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 512kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 522kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 532kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 542kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 552kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 563kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 573kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 583kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 593kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 604kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 614kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 624kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 634kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 645kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 655kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 665kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 675kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 686kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 696kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 706kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 716kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 727kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 737kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 747kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 757kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 768kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 778kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 788kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 798kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 808kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 819kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 829kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 839kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 849kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 860kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 870kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 880kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 890kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 901kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 911kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 921kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 931kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 942kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 952kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 962kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 972kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 983kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 993kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 43.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 39.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 43.3MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f728bd5d1a37826af750e62ad4b12fc4e8d36399bf4d943579f0a7ba23594b7d\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9w-FKL1SY3us","executionInfo":{"status":"ok","timestamp":1602169152353,"user_tz":240,"elapsed":14267,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["import glob\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, BertModel, BertForSequenceClassification"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2KFEvn-ovIS","executionInfo":{"status":"ok","timestamp":1602169177055,"user_tz":240,"elapsed":38960,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"0510cf79-249a-4f74-83ad-8fbc5db98c32","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6hFmok8gY3uw","executionInfo":{"status":"ok","timestamp":1602169178755,"user_tz":240,"elapsed":40651,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"49ea05c1-0eb4-4ce4-b9cc-b92f29afbe13","colab":{"base_uri":"https://localhost:8080/","height":217,"referenced_widgets":["65a96d49d3224cc3ab528950ee4e9c81","a192caaa2f584679b6a7eaca1186ce85","23c2f980f1d4474b99aaa81177d3506a","87f578f9436b40febc823c58d31c393d","1e1e4d2d246e4e8daebc5b1c77badd31","2dc536845439483e850beb92f836a9a6","6d664083c5ff4b80882e0356d71f32a1","28658b1ed31d4fb88532c162d8e9515b","4dc8618e4d354fdead0dc77dd5f11c1d","5fb3184400fb40d182fff22e121e02c3","b95fc9f871f94612b9258c1d8bce1da9","25e8d6ab6700456592f5718b787a2293","44fb43dd4482486ab9c255ddda21a536","f3d1e2c59b134b87a0903c7e09ee8ce0","4b6e467fa3cb46eaa8216c04589c16de","97b37eec53b746bd98cce27a3ce9b201"]}},"source":["# Test sentence\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","input_sentence = \"Hello my name is Jin\"\n","encoded = tokenizer(input_sentence)\n","print('Input sentence: ', input_sentence, '\\n')\n","print('Encoded: ', encoded, '\\n')\n","print('Decoded: ', tokenizer.decode(encoded['input_ids']), '\\n')\n"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65a96d49d3224cc3ab528950ee4e9c81","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dc8618e4d354fdead0dc77dd5f11c1d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Input sentence:  Hello my name is Jin \n","\n","Encoded:  {'input_ids': [101, 7592, 2026, 2171, 2003, 9743, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]} \n","\n","Decoded:  [CLS] hello my name is jin [SEP] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"90Uxq8Favedg"},"source":["## Get Preprocessed Review Data"]},{"cell_type":"code","metadata":{"id":"I2aD5wnzY3uz","executionInfo":{"status":"ok","timestamp":1602169179554,"user_tz":240,"elapsed":41441,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["# Get all review files\n","file_path = '/content/drive/My Drive/BERT project/Reviews/*[0-9].csv'\n","files = glob.glob(file_path)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJQx0DmbY3u2","executionInfo":{"status":"ok","timestamp":1602169196533,"user_tz":240,"elapsed":58411,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"810b0875-1b1f-4f05-e39d-0541e8a96c36","colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["# Concat all review data from different products into one big dataframe\n","df_list = []\n","\n","for file in files:\n","    df = pd.read_csv(file)\n","    df_list.append(df)\n","    \n","df = pd.concat(df_list, axis=0, ignore_index=True)\n","df"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>stars</th>\n","      <th>verified</th>\n","      <th>date</th>\n","      <th>country</th>\n","      <th>helpful</th>\n","      <th>has-media</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I could sit here and write all about the specs...</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2019-6-25</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A very reasonably priced laptop for basic comp...</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2019-7-2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>This is the best laptop deal you can get, full...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2019-8-3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A few months after the purchase....It is still...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2019-7-12</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>BUYER BE AWARE: This computer has Microsoft 10...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2019-8-7</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>120384</th>\n","      <td>Great overall product</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2019-12-28</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>120385</th>\n","      <td>wow~^^very good</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2019-12-30</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>120386</th>\n","      <td>The sound is amazing!</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2020-1-9</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>120387</th>\n","      <td>I love my AirPods Pro</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2019-11-23</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>120388</th>\n","      <td>Amazing sound quality</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2020-3-1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>120389 rows × 7 columns</p>\n","</div>"],"text/plain":["                                                  comment  ...  has-media\n","0       I could sit here and write all about the specs...  ...          0\n","1       A very reasonably priced laptop for basic comp...  ...          1\n","2       This is the best laptop deal you can get, full...  ...          1\n","3       A few months after the purchase....It is still...  ...          0\n","4       BUYER BE AWARE: This computer has Microsoft 10...  ...          0\n","...                                                   ...  ...        ...\n","120384                              Great overall product  ...          0\n","120385                                    wow~^^very good  ...          0\n","120386                              The sound is amazing!  ...          0\n","120387                              I love my AirPods Pro  ...          0\n","120388                              Amazing sound quality  ...          0\n","\n","[120389 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"N-KPuXLqvoYU"},"source":["## Data Exploration"]},{"cell_type":"code","metadata":{"id":"ObzN_HRMY3u5","executionInfo":{"status":"ok","timestamp":1602169196534,"user_tz":240,"elapsed":58400,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"19f47777-0ee3-4fd6-dca4-858fd9b63db4","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Check for missing values\n","df.isnull().any()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["comment       True\n","stars        False\n","verified     False\n","date         False\n","country      False\n","helpful      False\n","has-media    False\n","dtype: bool"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"-T8IWcfnY3u7","executionInfo":{"status":"ok","timestamp":1602169196535,"user_tz":240,"elapsed":58391,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"fbf30645-c913-4315-9a58-a97b3f210751","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Find reviews with missing comments\n","missing_indices = df[df['comment'].isnull()].index.tolist()\n","print('Number of reviews missing comments: ', len(missing_indices))\n","print('Missing indices: ', missing_indices)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Number of reviews missing comments:  17\n","Missing indices:  [24835, 37237, 37277, 40072, 50852, 64895, 69760, 80854, 81562, 84103, 86420, 92670, 98539, 98769, 105389, 105405, 112139]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EPVAW6eY3vA","executionInfo":{"status":"ok","timestamp":1602169196536,"user_tz":240,"elapsed":58381,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"4a211221-c9f2-4575-de7a-34a71d899ca3","colab":{"base_uri":"https://localhost:8080/"}},"source":["print('Max comment length (of all products): ', int(df.comment.str.len().max()))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Max comment length (of all products):  5127\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u20PO3tZY3vD","executionInfo":{"status":"ok","timestamp":1602170265227,"user_tz":240,"elapsed":823,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"32ea2cd3-c1ad-491c-b4ec-8f9c743e5690","colab":{"base_uri":"https://localhost:8080/","height":504}},"source":["# Get only the comments and star (labels) data\n","df = df[['comment', 'stars']]\n","df.dropna(inplace=True)\n","df"],"execution_count":46,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>stars</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I could sit here and write all about the specs...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A very reasonably priced laptop for basic comp...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>This is the best laptop deal you can get, full...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A few months after the purchase....It is still...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>BUYER BE AWARE: This computer has Microsoft 10...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>120384</th>\n","      <td>Great overall product</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>120385</th>\n","      <td>wow~^^very good</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>120386</th>\n","      <td>The sound is amazing!</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>120387</th>\n","      <td>I love my AirPods Pro</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>120388</th>\n","      <td>Amazing sound quality</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>120372 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  comment  stars\n","0       I could sit here and write all about the specs...      5\n","1       A very reasonably priced laptop for basic comp...      4\n","2       This is the best laptop deal you can get, full...      5\n","3       A few months after the purchase....It is still...      5\n","4       BUYER BE AWARE: This computer has Microsoft 10...      1\n","...                                                   ...    ...\n","120384                              Great overall product      5\n","120385                                    wow~^^very good      5\n","120386                              The sound is amazing!      5\n","120387                              I love my AirPods Pro      5\n","120388                              Amazing sound quality      5\n","\n","[120372 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"mSeS1SsRY3vF","executionInfo":{"status":"ok","timestamp":1602170269194,"user_tz":240,"elapsed":808,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"27485368-f982-4746-af4a-8b31d20f78d9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# print first 5 reviews\n","for idx, row in comment_df[:5].iterrows():\n","    print(row['comment'] + '\\n')"],"execution_count":47,"outputs":[{"output_type":"stream","text":["I could sit here and write all about the specs on this computer, but they are already in the description, and If you are like me... you don't really understand it anyways.So I am going to tell you what I LOVE about this computer and what I use it for. I am a full time college student as well as a single mother who stays busy. I have previously used a HP All In one computer that I bought brand new a year ago and I hate that thing... It is so slow!!! When I first opened this item, I was just hoping that it would be a little faster! What I got instead was an amazing computer that is faster than I could have ever imagined. Now I don't use this thing for much more than amazon reviews, school work, and papers. But this is exactly what I needed.\n","\n","A very reasonably priced laptop for basic computing needs. The specs that stick out to me for describing this as \"basic needs\" is 4GB of RAM, and 128GB M.2 SSD. Both are at the bare minimum in today's needs. Cell phones now come with those specs( high dollar ones though..).Seems like one of the goals of this laptop is to keep an aesthetically pleasing design. Just 1 3.1 USB port and 2 2.0 USB ports are included, one headphone/mic jack, 1 slim RJ45 port, HDMI, and power input plug. No external battery.\n","\n","This is the best laptop deal you can get, full stop.Touchescreen? Nope. That feature is stupid in a laptop, in my view. As you use the keyboard, incidental touche s of the screen change focus or move to different screens--touchscreen is a gimmick and sucks.\n","\n","A few months after the purchase....It is still running good and I bought 5 more for my colleagues, so I bought all together 8 machines.  My of my colleagues need more memory while some need less.  So what I do is, for those who need more, I took the 4GB RAM out from their machine and put it in the machine of the colleague who needs less.  So 1 machine got 2x4G running in dual channel with identical RAM and install a new 2x8G for the original machine.  To be honest, difference for 4 to 8 is high, but little difference from 8 to 16.  So those in a tight budget can save your money.\n","\n","BUYER BE AWARE: This computer has Microsoft 10S. This is scam software by Microsoft to force you to use only Microsoft apps from their deserted island wasteland of an app store. The computer will not allow you to do anything else. You can upgrade to a full version of Microsoft 10... for another $134. Hard pass. STAY AWAY from ALL computers with Microsoft 10S!! Absolute garbage software.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gm94ltcsv0A4"},"source":["## Define the Reviews Dataset\n","Each item in the dataset will return a dictionary consisting of:\n","\n","\n","*   input_ids: the input token ids\n","*   attn_mask: the attention mask of the input sequence\n","*   label: the target star rating of the input review"]},{"cell_type":"code","metadata":{"id":"pbIXeGa0Y3vH","executionInfo":{"status":"ok","timestamp":1602172240914,"user_tz":240,"elapsed":802,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["class ReviewsDataset(Dataset):\n","    def __init__(self, df, max_length=1024):\n","        self.df = df\n","        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length \n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        # input=review, label=stars\n","        review = self.df.loc[idx, 'comment']\n","        # labels are 0-indexed\n","        label = int(self.df.loc[idx, 'stars']) - 1\n","        \n","        encoded = self.tokenizer(\n","            review,                      # review to encode\n","            add_special_tokens=True,\n","            max_length=self.max_length,  # Truncate all segments to max_length\n","            padding='max_length',        # pad all reviews with the [PAD] token to the max_length\n","            return_attention_mask=True,  # Construct attention masks.\n","            return_tensors='pt'\n","        )\n","        \n","        input_ids = encoded['input_ids']\n","        attn_mask = encoded['attention_mask']\n","        \n","        return {\n","            'input_ids': torch.tensor(input_ids),\n","            'attn_mask': torch.tensor(attn_mask), \n","            'label': torch.tensor(label)\n","        }"],"execution_count":120,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDymyf0zvFt0"},"source":["Define some constants that are important later on."]},{"cell_type":"code","metadata":{"id":"6R4CqyQMY3vI","executionInfo":{"status":"ok","timestamp":1602172242853,"user_tz":240,"elapsed":827,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["MAX_LEN = 1024\n","TRAIN_BATCH_SIZE = 64\n","VALID_BATCH_SIZE = 32\n","EPOCHS = 1\n","LEARNING_RATE = 1e-05"],"execution_count":121,"outputs":[]},{"cell_type":"code","metadata":{"id":"sg4ed5sNY3vK","executionInfo":{"status":"ok","timestamp":1602172243116,"user_tz":240,"elapsed":712,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"797bfbd3-e517-4892-974a-4a98ceace1cb","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"execution_count":122,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"markdown","metadata":{"id":"B0p0Abyju3iT"},"source":["## Create Datasets / DataLoaders\n","Create the train and test datasets and dataloaders for the neural network."]},{"cell_type":"code","metadata":{"id":"fP9HLK63sdji","executionInfo":{"status":"ok","timestamp":1602172246619,"user_tz":240,"elapsed":3302,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"c2482ca6-e85b-42fc-9469-ff9fbdeff625","colab":{"base_uri":"https://localhost:8080/"}},"source":["train_size = 0.8\n","train_dataset=df.sample(frac=train_size,random_state=200)\n","test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","training_set = ReviewsDataset(train_dataset, MAX_LEN)\n","testing_set = ReviewsDataset(test_dataset, MAX_LEN)\n","\n","print(\"# of samples in train set: {}\".format(len(training_set)))\n","print(\"# of samples in test set: {}\".format(len(testing_set)))"],"execution_count":123,"outputs":[{"output_type":"stream","text":["# of samples in train set: 96298\n","# of samples in test set: 24074\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9L9c9iFDY3vM","executionInfo":{"status":"ok","timestamp":1602172246620,"user_tz":240,"elapsed":2846,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["train_params = {\n","                'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {\n","                'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"execution_count":124,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3mQyVauw6At"},"source":["## Define the neural model for fine tuning\n","Given a review as an input sequence, we want to predict its star rating. This is a multi-class sequence classification task.\n","\n","For out model, we will use BertForSequenceClassification and set the num_labels argument to the number of unique values for Amazon star ratings."]},{"cell_type":"code","metadata":{"id":"ckkUd3eVw8_3","executionInfo":{"status":"ok","timestamp":1602172251439,"user_tz":240,"elapsed":6794,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"193a6f34-f249-4ae6-d7ee-49cc8129345a","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels = len(df['stars'].unique()), # number of unique labels for our multi-class classification problem\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")"],"execution_count":125,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VfNsRkv2z-11"},"source":["## Fine Tuning the Model on Train Dataset"]},{"cell_type":"code","metadata":{"id":"QySwvdEfuyQw","executionInfo":{"status":"ok","timestamp":1602172251440,"user_tz":240,"elapsed":6010,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["# Define the optimizer\n","loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"],"execution_count":126,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TWcRWb5txai","executionInfo":{"status":"ok","timestamp":1602172251441,"user_tz":240,"elapsed":5530,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["# Define the accuracy function\n","def calcuate_accuracy(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"],"execution_count":127,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMpY_1OgzG3K","executionInfo":{"status":"ok","timestamp":1602172251442,"user_tz":240,"elapsed":4888,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}}},"source":["def train(epoch):\n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    model.train()\n","    for _, data in enumerate(training_loader, 0):\n","        input_ids = data['input_ids'].to(device)\n","        mask = data['attn_mask'].to(device)\n","        labels = data['label'].to(device)\n","\n","        print(input_ids)\n","        print(labels)\n","        print(mask)\n","        \n","        print('\\n', input_ids.shape)\n","        print(labels.shape)\n","        print(mask.shape, '\\n')\n","        \n","        loss, outputs = model(\n","                              input_ids, \n","                              token_type_ids=None,\n","                              attention_mask=mask,\n","                              )\n","        break\n","\n","        outputs = model(ids, mask)\n","        loss = loss_function(outputs, labels)\n","        tr_loss += loss.item()\n","        # gets labels with highest probabilities and their corresponding indices\n","        big_val, big_idx = torch.max(outputs.data, dim=1)\n","        n_correct += calcuate_accuracy(big_idx, labels)\n","\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","        \n","        if _%5000==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            accu_step = (n_correct*100)/nb_tr_examples \n","            print(f\"Training Loss per 5000 steps: {loss_step}\")\n","            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # When using GPU\n","        optimizer.step()\n","\n","    # print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n","    # epoch_loss = tr_loss/nb_tr_steps\n","    # epoch_accu = (n_correct*100)/nb_tr_examples\n","    # print(f\"Training Loss Epoch: {epoch_loss}\")\n","    # print(f\"Training Accuracy Epoch: {epoch_accu}\")\n","\n","    return"],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVnQNlnS02sS","executionInfo":{"status":"error","timestamp":1602172252497,"user_tz":240,"elapsed":5215,"user":{"displayName":"Jin Koay","photoUrl":"","userId":"04887700171803706985"}},"outputId":"773c7633-e388-4389-dca9-73c07d49abb5","colab":{"base_uri":"https://localhost:8080/","height":990}},"source":["model.to(device)\n","for epoch in range(EPOCHS):\n","    train(epoch)\n","    break"],"execution_count":129,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["tensor([[[  101,  2572,  4039,  ...,     0,     0,     0]],\n","\n","        [[  101,  2204,  2132,  ...,     0,     0,     0]],\n","\n","        [[  101,  2573,  2092,  ...,     0,     0,     0]],\n","\n","        ...,\n","\n","        [[  101,  1045,  2074,  ...,     0,     0,     0]],\n","\n","        [[  101,  2750,  1037,  ...,     0,     0,     0]],\n","\n","        [[  101,  2009, 14057,  ...,     0,     0,     0]]], device='cuda:0')\n","tensor([2, 4, 4, 0, 4, 4, 4, 4, 0, 0, 0, 4, 4, 0, 3, 3, 4, 2, 4, 4, 0, 4, 4, 4,\n","        4, 4, 4, 0, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        3, 3, 4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 3, 0, 2, 2], device='cuda:0')\n","tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        ...,\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]]], device='cuda:0')\n","\n"," torch.Size([64, 1, 1024])\n","torch.Size([64])\n","torch.Size([64, 1, 1024]) \n","\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-129-a05f9b83ec8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-128-ccae94818d5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m                               \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                               \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                               \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                               )\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m         )\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m                 )\n\u001b[1;32m    484\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         )\n\u001b[1;32m    341\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_key_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_value_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     def forward(\n","\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"]}]},{"cell_type":"code","metadata":{"id":"U4YENiejwLN7"},"source":[""],"execution_count":null,"outputs":[]}]}