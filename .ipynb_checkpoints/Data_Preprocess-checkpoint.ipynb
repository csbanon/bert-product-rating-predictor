{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BertTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ef4e4fc82b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello my name is Jin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Encoded: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BertTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test sentence\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "input_sentence = \"Hello my name is Jin\"\n",
    "encoded = tokenizer(input_sentence)\n",
    "print('Input sentence: ', input_sentence, '\\n')\n",
    "print('Encoded: ', encoded, '\\n')\n",
    "print('Decoded: ', tokenizer.decode(encoded['input_ids']), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all review files\n",
    "files = glob.glob(r'./Reviews/*[0-9].csv')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all review data from different products into one big dataframe\n",
    "df_list = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "    \n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find reviews with missing comments\n",
    "missing_indices = df[df['comment'].isnull()].index.tolist()\n",
    "print('Number of reviews missing comments: ', len(missing_indices))\n",
    "print('Missing indices: ', missing_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what an example review with missing comment looks like\n",
    "print(df.iloc[120116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max comment length (of all products): ', int(df.comment.str.len().max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the comments and star (labels) data\n",
    "comment_df = df[['comment', 'stars']]\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 reviews\n",
    "for idx, row in comment_df[:5].iterrows():\n",
    "    print(row['comment'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset:\n",
    "    def __init__(self, df, max_length=1024):\n",
    "        self.df = df\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # input=review, label=rating\n",
    "        review = self.df.loc[idx, 'comment']\n",
    "        rating = int(self.df.loc[idx, 'stars'])\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            review,                      # review to encode\n",
    "            max_length=self.max_length,  # Truncate all segments to max_length\n",
    "            padding='max_length',        # pad all reviews with the [PAD] token to the max_length\n",
    "            return_attention_mask=True,  # Construct attention masks.\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids']\n",
    "        attn_mask = encoded['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attn_mask': attn_mask, \n",
    "            'rating': rating\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = ReviewsDataset(df, 1024)\n",
    "data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
